{"cells":[{"cell_type":"code","source":["from tenacity import retry, stop_never, wait_fixed, retry_if_exception\n","from pyspark.sql.types import StructType, StructField, StringType\n","from pyspark.sql.functions import lit"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"62daa86b-3829-4c13-a613-e7d7325ad4ca","normalized_state":"finished","queued_time":"2025-12-23T06:42:57.4362954Z","session_start_time":null,"execution_start_time":"2025-12-23T06:43:03.0702919Z","execution_finish_time":"2025-12-23T06:43:03.4079554Z","parent_msg_id":"107ef985-57ad-4738-a3b9-0f5273811efa"},"text/plain":"StatementMeta(, 62daa86b-3829-4c13-a613-e7d7325ad4ca, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"289ab9f3-d59a-4d68-8d6b-c8064180b92e"},{"cell_type":"code","source":["# CONSTANTS\n","SERVER_NAME='dwpbgshdgicevmv2xhwrfh6yam-hgg4xv4ptkse7pco4qgre7rpri.datawarehouse.fabric.microsoft.com'\n","DATABASE_NAME='sample_warehouse'\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"62daa86b-3829-4c13-a613-e7d7325ad4ca","normalized_state":"finished","queued_time":"2025-12-23T06:42:58.1793187Z","session_start_time":null,"execution_start_time":"2025-12-23T06:43:03.4098653Z","execution_finish_time":"2025-12-23T06:43:03.7203223Z","parent_msg_id":"7801eb78-a57c-4440-903f-4717ac2bfb16"},"text/plain":"StatementMeta(, 62daa86b-3829-4c13-a613-e7d7325ad4ca, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c53fda4-8554-4281-b703-cb773295036d"},{"cell_type":"code","source":["# -----------------------------------------------------------\n","# 1. Execute Dynamic SQL Against Fabric Warehouse\n","# -----------------------------------------------------------\n","def execute_dynamic_dw_query(query: str) -> None:\n","    \"\"\"\n","    Executes a SQL statement dynamically through JDBC using Spark's JRE.\n","    This function is used for all ETL metadata updates.\n","\n","    Parameters:\n","        query (str): SQL query to execute.\n","    \"\"\"\n","\n","    server_name = SERVER_NAME\n","    database_name = DATABASE_NAME\n","    port = 1433\n","\n","    jdbc_url = f\"jdbc:sqlserver://{server_name}:{port};database={database_name}\"\n","\n","    # Authentication token for SQL Endpoint (Fabric)\n","    token = mssparkutils.credentials.getToken(\n","        'https://analysis.windows.net/powerbi/api'\n","    )\n","\n","    # JDBC connection properties\n","    java_props = spark._jvm.java.util.Properties()\n","    java_props.setProperty(\"accessToken\", token)\n","    java_props.setProperty(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n","\n","    # Create JDBC connection\n","    conn = spark._jvm.java.sql.DriverManager.getConnection(jdbc_url, java_props)\n","\n","    stmt = conn.createStatement()\n","    # stmt.execute(query)\n","    has_result_set = stmt.execute(query)\n","    max_rows =20\n","\n","    if has_result_set:\n","        rs = stmt.getResultSet()\n","        rs_meta = rs.getMetaData()\n","        col_count = rs_meta.getColumnCount()\n","\n","        # Print column headers\n","        columns = [rs_meta.getColumnName(i) for i in range(1, col_count + 1)]\n","        print(\" | \".join(columns))\n","        print(\"-\" * 80)\n","\n","        row_count = 0\n","        while rs.next() and row_count < max_rows:\n","            row = [str(rs.getObject(i)) for i in range(1, col_count + 1)]\n","            print(\" | \".join(row))\n","            row_count += 1\n","\n","        if row_count == max_rows:\n","            print(f\"... output truncated after {max_rows} rows\")\n","\n","        rs.close()\n","    else:\n","        # For UPDATE / INSERT / DELETE\n","        update_count = stmt.getUpdateCount()\n","        print(f\"Statement executed successfully. Rows affected: {update_count}\")\n","\n","    stmt.close()\n","    conn.close()\n","\n","    print(f\"SQL executed successfully: {query}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"62daa86b-3829-4c13-a613-e7d7325ad4ca","normalized_state":"finished","queued_time":"2025-12-23T06:45:59.3457541Z","session_start_time":null,"execution_start_time":"2025-12-23T06:45:59.3468056Z","execution_finish_time":"2025-12-23T06:45:59.6743733Z","parent_msg_id":"71920c6a-8b08-44e1-bb94-5e05c014564b"},"text/plain":"StatementMeta(, 62daa86b-3829-4c13-a613-e7d7325ad4ca, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f17a1714-da57-436b-8390-a02c96ccc371"},{"cell_type":"code","source":["# query = \"\"\"SELECT TOP (100) [GeographyID],\n","# \t\t\t[ZipCodeBKey],\n","# \t\t\t[County],\n","# \t\t\t[City],\n","# \t\t\t[State],\n","# \t\t\t[Country],\n","# \t\t\t[ZipCode]\n","# FROM [sample_warehouse].[dbo].[Geography]\"\"\"\n","\n","# execute_dynamic_dw_query(query)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"62daa86b-3829-4c13-a613-e7d7325ad4ca","normalized_state":"finished","queued_time":"2025-12-23T06:46:04.9143273Z","session_start_time":null,"execution_start_time":"2025-12-23T06:46:04.91546Z","execution_finish_time":"2025-12-23T06:46:06.3361293Z","parent_msg_id":"bbe73571-880f-475b-a1fe-611e0667749c"},"text/plain":"StatementMeta(, 62daa86b-3829-4c13-a613-e7d7325ad4ca, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["GeographyID | ZipCodeBKey | County | City | State | Country | ZipCode\n--------------------------------------------------------------------------------\n10 | 90001 | Manhattan County | New York | NY | USA | 10010\n1 | 10001 | Manhattan County | New York | NY | USA | 10001\n2 | 10002 | Manhattan County | New York | NY | USA | 10002\n3 | 10003 | Manhattan County | New York | NY | USA | 10003\n4 | 10004 | Manhattan County | New York | NY | USA | 10004\n5 | 10005 | Manhattan County | New York | NY | USA | 10005\n6 | 10006 | Manhattan County | New York | NY | USA | 10006\n7 | 10007 | Manhattan County | New York | NY | USA | 10007\n8 | 10008 | Manhattan County | New York | NY | USA | 10008\n9 | 10009 | Manhattan County | New York | NY | USA | 10009\n11 | 11201 | Kings County | New York | NY | USA | 11201\n12 | 11202 | Kings County | New York | NY | USA | 11202\n13 | 11203 | Kings County | New York | NY | USA | 11203\n14 | 11204 | Kings County | New York | NY | USA | 11204\n15 | 11205 | Kings County | New York | NY | USA | 11205\n16 | 11206 | Kings County | New York | NY | USA | 11206\n17 | 11207 | Kings County | New York | NY | USA | 11207\n18 | 11208 | Kings County | New York | NY | USA | 11208\n19 | 11209 | Kings County | New York | NY | USA | 11209\n20 | 11210 | Kings County | New York | NY | USA | 11210\n... output truncated after 20 rows\nSQL executed successfully: SELECT TOP (100) [GeographyID],\n\t\t\t[ZipCodeBKey],\n\t\t\t[County],\n\t\t\t[City],\n\t\t\t[State],\n\t\t\t[Country],\n\t\t\t[ZipCode]\nFROM [sample_warehouse].[dbo].[Geography]\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"84527fdd-a1d5-46d2-b11f-cc2f40562bc2"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"47788ba0-b06a-4a0a-be47-10ab922e455e"},{"cell_type":"code","source":["# -----------------------------------------------------------\n","# 2. Helper Functions for Exception Detection\n","# -----------------------------------------------------------\n","def is_snapshot_isolation(exception):\n","    \"\"\"Identify snapshot-isolation conflict errors.\"\"\"\n","    return \"Snapshot isolation\" in str(exception)\n","\n","\n","def is_invalid_object_exception(exception):\n","    \"\"\"Detect 'table not found' errors on the SQL Endpoint.\"\"\"\n","    return \"Invalid object name\" in str(exception)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"62daa86b-3829-4c13-a613-e7d7325ad4ca","normalized_state":"finished","queued_time":"2025-12-23T06:46:29.7343289Z","session_start_time":null,"execution_start_time":"2025-12-23T06:46:29.7353729Z","execution_finish_time":"2025-12-23T06:46:30.0404527Z","parent_msg_id":"56322a4f-42d2-4b1d-9bd1-0cf579ac8653"},"text/plain":"StatementMeta(, 62daa86b-3829-4c13-a613-e7d7325ad4ca, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"698f961e-ed4f-49a8-86cf-5a0ce5d7eb77"},{"cell_type":"code","source":["# -----------------------------------------------------------\n","# 3. Update ETL_TASK_EXECUTION_LOG with Retry on Snapshot Errors\n","# -----------------------------------------------------------\n","@retry(\n","    stop=stop_never,          # Retry indefinitely\n","    wait=wait_fixed(20),      # 20 sec pause between attempts\n","    retry=retry_if_exception(is_snapshot_isolation),\n",")\n","def retryable_update(run_id, task_ids, status, task_error_message=None, predecessor_task_id=None):\n","    \"\"\"\n","    Updates ETL_TASK_EXECUTION_LOG with retry logic for snapshot isolation conflicts.\n","    This ensures ETL metadata updates succeed even under high concurrency.\n","\n","    Parameters:\n","        run_id (int): Instance ID\n","        task_ids (int | list): One or multiple task IDs\n","        status (str): SUCCESS / FAILED / BLOCKED\n","        task_error_message (str): Optional error message\n","    \"\"\"\n","\n","    # Normalize task_ids to always be a list\n","    if isinstance(task_ids, int):\n","        task_ids = [task_ids]\n","    elif not isinstance(task_ids, list):\n","        raise ValueError(\"task_ids should be an int or a list\")\n","\n","    task_id_str = \", \".join(map(str, task_ids))\n","\n","    # Build TASK_ERROR_MESSAGE\n","    if status == \"FAILED\":\n","        task_error_message = f\"'{task_error_message}'\" if task_error_message else \"NULL\"\n","    elif status == \"BLOCKED\":\n","        task_error_message = f\"'PREDECESSOR TASK {predecessor_task_id} FAILED'\"\n","    else:\n","        task_error_message = \"NULL\"\n","\n","    sql = f\"\"\"\n","        UPDATE dbo.ETL_TASK_EXECUTION_LOG\n","        SET END_TIME = CURRENT_TIMESTAMP,\n","            STATUS = '{status}',\n","            TASK_ERROR_MESSAGE = {task_error_message}\n","        WHERE INSTANCE_ID = {run_id}\n","        AND TASK_ID IN ({task_id_str})\n","    \"\"\"\n","\n","    print(f\"Generated SQL: {sql}\")\n","\n","    try:\n","        execute_dynamic_dw_query(sql)\n","\n","    except Exception as e:\n","        if is_snapshot_isolation(str(e)):\n","            # Trigger retry in Tenacity\n","            print(f\"Snapshot isolation conflict detected for tasks {task_ids}. Retrying...\")\n","            raise Exception(str(e))\n","\n","        # Fallback update on unexpected errors\n","        fallback_sql = f\"\"\"\n","            UPDATE dbo.ETL_TASK_EXECUTION_LOG\n","            SET END_TIME = CURRENT_TIMESTAMP,\n","                STATUS = '{status}',\n","                TASK_ERROR_MESSAGE = 'FAILED WHILE UPDATING METADATA'\n","            WHERE INSTANCE_ID = {run_id}\n","            AND TASK_ID IN ({task_id_str})\n","        \"\"\"\n","        print(\"Executing fallback update...\")\n","        execute_dynamic_dw_query(fallback_sql)\n","\n","    print(f\"Updated task execution log for Run ID {run_id}, Task(s) {task_ids}.\")\n","\n","\n","# -----------------------------------------------------------\n","# 4. Check if Lakehouse Temp Table Appears on SQL Endpoint + Update Params Table\n","# -----------------------------------------------------------\n","@retry(\n","    stop=stop_never,\n","    wait=wait_fixed(20),\n","    retry=retry_if_exception(lambda e: is_invalid_object_exception(e) or is_snapshot_isolation(e)),\n",")\n","def check_for_lakehousetable_update_params_table(instance_id, task_id):\n","    \"\"\"\n","    Polls SQL Endpoint repeatedly until DYNA_PARAMS_xx_xx table becomes available.\n","    Then updates ETL_PARAMS using the data.\n","\n","    Handles both:\n","      - SQL Endpoint propagation delay\n","      - Snapshot isolation conflicts\n","    \"\"\"\n","\n","    print(\"Checking if dynamic params table is available in SQL Endpoint...\")\n","\n","    staging_table = f\"sample_lakehouse.dbo.DYNA_PARAMS_{instance_id}_{task_id}\"\n","\n","    # Check if table exists\n","    execute_dynamic_dw_query(f\"SELECT * FROM {staging_table}\")\n","    print(\"Dynamic params table detected. Proceeding with ETL_PARAMS update.\")\n","\n","    # Update ETL_PARAMS table\n","    sql = f\"\"\"\n","        UPDATE dbo.[ETL_PARAMS]\n","        SET LAST_UPDATED_DATE = CAST(GETDATE() AS DATE),\n","            PREV_PARAMETER_VALUE = stg.PREV_PARAM_VALUE,\n","            PARAMETER_VALUE = stg.CUR_PARAM_VALUE\n","        FROM {staging_table} stg\n","        INNER JOIN dbo.[ETL_PARAMS] tg\n","            ON stg.PLAN_ID = tg.PLAN_ID\n","            AND stg.TASK_ID = tg.TASK_ID\n","            AND stg.PARAMETER_NAME = tg.PARAMETER_NAME\n","        WHERE tg.PARAMETER_TYPE = 'dynamic';\n","    \"\"\"\n","\n","    print(f\"Generated Update SQL: {sql}\")\n","    execute_dynamic_dw_query(sql)\n","\n","    print(f\"ETL_PARAMS updated successfully for Run ID {instance_id}, Task {task_id}.\")\n","\n","\n","# -----------------------------------------------------------\n","# 5. Save Dynamic Params to Lakehouse → Wait for Sync → Update Warehouse → Clean Up\n","# -----------------------------------------------------------\n","def save_dynamic_params(op_json, instance_id, task_id, plan_id):\n","    \"\"\"\n","    Writes dynamic parameters to Lakehouse temp folder,\n","    waits for the data to sync to SQL Endpoint,\n","    updates ETL_PARAMS table,\n","    and removes temporary location.\n","\n","    Parameters:\n","        op_json: Parsed JSON containing dyna_params array\n","        instance_id (int)\n","        task_id (int)\n","        plan_id (int)\n","    \"\"\"\n","\n","    print(f\"Creating dataframe for dynamic parameters: {op_json['dyna_params']}\")\n","\n","    # Define schema for dynamic parameters\n","    columns = ['PLAN_ID', 'TASK_ID', 'PARAMETER_NAME', 'PREV_PARAM_VALUE', 'CUR_PARAM_VALUE']\n","    schema = StructType([StructField(c, StringType(), True) for c in columns])\n","\n","    df = spark.createDataFrame(op_json['dyna_params'], schema)\n","    df = df.withColumn(\"TASK_ID\", lit(task_id)).withColumn(\"PLAN_ID\", lit(plan_id))\n","\n","    lakehouse_path = f\"{METADATA_LAKEHOUSE_TEMP_LOC}/DYNA_PARAMS_{instance_id}_{task_id}\"\n","\n","    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"True\").save(lakehouse_path)\n","\n","    print(f\"Written dynamic params to temp Lakehouse location: {lakehouse_path}\")\n","\n","    # Wait for SQL Endpoint sync → Then update ETL_PARAMS\n","    check_for_lakehousetable_update_params_table(instance_id, task_id)\n","\n","    # Cleanup Lakehouse temp folder\n","    print(f\"Removing temporary Lakehouse folder: {lakehouse_path}\")\n","    notebookutils.fs.rm(lakehouse_path, True)\n","\n","    print(\"Dynamic parameter update completed successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"62daa86b-3829-4c13-a613-e7d7325ad4ca","normalized_state":"finished","queued_time":"2025-12-23T06:46:32.8146287Z","session_start_time":null,"execution_start_time":"2025-12-23T06:46:32.8158816Z","execution_finish_time":"2025-12-23T06:46:33.298606Z","parent_msg_id":"9288e67f-172e-4d45-aa2c-64c6795b76b2"},"text/plain":"StatementMeta(, 62daa86b-3829-4c13-a613-e7d7325ad4ca, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ae0c120c-1206-46b1-8ebe-4e971eed94fa"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}